> [1] The title is witty but unnecessary. The authors should be mindful that the journal is read by an international audience and a simple and straightforward title would be more appealing. 

We are saddened to hear that Shakespeare's appeal is not international.

> [2] The authors provide a thorough and detailed description of the limitations provided by existing normalization methods - in particular DEseq2 and TMM. However, these methods were never designed with single cell data processing so although it is useful to briefly highlight their ineffectiveness with single cell zero count data, I am not sure the authors need to devote so much of the paper to its discussion. It takes nearly six pages and two figures before we reach the interesting bit which is their proposed method.

It is true that DESeq2 and TMM normalization were not designed for single-cell data. 
Regardless, many single-cell RNA-seq studies have continued to use these methods in their analysis piplines -- or even more simply, normalized by the total library size without considering the effect of DE.
Examples of such studies include Saraiva et al. (2015), Kolodziejczyk et al. (2015), Li et al. (2015), Deng et al. (2014) and Freeman et al. (2015), and we have added these citations to the text.
We feel that it is important to describe the limitations of these methods in depth, as their lack of accuracy will be relevant to the broader scRNA-seq data analysis community. 

> [3] The authors proposed one possible pooling strategy. How much of an effect does the choice of pooling have on size factor estimates? A statement should be made on this issue or a simulation study to investigate the stability of the size factor estimates with respect to pooling selection should be performed.

In the initial design of the deconvolution method, we randomly selected pools of cells to construct the linear system.
To our surprise, this actually performed quite well. 
Eventually, though, we refined the method to use a ring arrangement, which provides a modest improvement in precision.
We have added some simulations to the Supplementary Materials showing a two-fold decrease in the median (log-)error when the ring is used instead of random pools.
Of course, there may be alternative pooling arrangements that perform even better, but we do not think this is a major issue, given that the current set-up provides satisfactory performance.

> [4] The deconvolution method requires the solution of a linear system in which their as many equations as cells. Is this computationally problematic for studies involving very large cell numbers? The authors could comment on the computational demands of their method.

This is a good point.
The runtime and memory requirements increase quadratically with the number of cells, due to the increase in the size of the linear system.
However, for large data sets, this can largely be mitigated by clustering and performing deconvolution within each cluster.
For example, clustering is performed and each cluster is around 200 cells, the size of the linear system is capped at about 200 cells.
Further increases in the number of cells will manifest as increasing numbers of clusters, for which the method is linear in runtime.
Of course, the clustering step itself is quadratic with respect to the number of cells, but we expect that clustering of cells would be performed anyway as a routine part of the scRNA-seq data analysis -- so, no additional time is added to the analysis by normalization.
We have added a description of this to the Supplementary Materials.

> [5] The authors use weighted least squares to give a point estimate of the size factors but the method would be much more powerful if uncertainty information could be computed and if this uncertainty could be propagated into downstream analysis.  

We agree that, in an ideal world, the uncertainty of the size factors would used in downstream analyses.
This would give more accurate inferences that account for errors in normalization.
However, in practice, this is not the case.
Pipelines like DESeq2 and edgeR that are commonly applied on scRNA-seq data assume that size/normalization factors are known values.
Similarly, monocle (a dedicated single-cell analysis method) requires normalized expression values as input, without any provision for the variability of the normalization procedure.
In general, a holistic framework is required to incorporate uncertainty into downstream inferences, e.g., using a Bayesian model as described by Vallejos et al. (2015).
This is not compatible with the majority of existing methods that are more modular.
As such, while it is possible to extract standard errors from the linear system, we do not do so as we cannot forsee their use in real data analyses.

> [6] A major difficulty that the authors face is demonstrating the effectiveness of the method in real world analysis situations. The authors offer some anecdotal evidence based on the analysis of two single cell data sets but ultimately the conclusion is little more than a statement that different size factor estimates can lead to different DE gene lists. I think more needs to be done to show the strength of the method in real world scenarios even if this is only defined statistically. For example, random subsets of the data could be taken and the DE gene lists compared across subsets to identify the genes that overlap. Does the deconvolution method lead to more stable gene lists that consistently report the same DE genes? This is merely a suggestion but something beyond a statement that "these results suggest that the genes unique to deconvolution are more biologically relevant that those unique to library size normalization" is required to make the analysis more credible.

We have added a simple GLM-based analysis to demonstrate the relative accuracy of the deconvolution method compared to other methods.
We use one set of (log-)size factors as GLM offsets, while we use another set of size factors as a covariate in the model.
We then fit a GLM to the counts for a homogenous set of cells in which no DE is expected (or at least, no DE correlated with the true size factors).
Any biases in the offsets will lead to a systematic difference between the fitted values and the (true) mean -- to compensate, the estimated coefficient for the covariate term will become non-zero.
Dropping the covariate will subsequently yield some DE genes.
In contrast, if there are no biases, there will be no systematic difference between the fitted value and the true mean.
This means that the covariate term will remain at zero, resulting in fewer DE genes.
We show that using deconvolution as an offset and each of the other methods as the covariate yields fewer DE genes than when the other methods are used as the offset and deconvolution is used as the covariate.
This suggests that deconvolution is indeed more accurate than the other methods.
We have added this analysis to the Supplementary Materials.
