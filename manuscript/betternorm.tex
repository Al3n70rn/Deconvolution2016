\documentclass{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black}
\urlstyle{same}

\title{Computing normalization factors for single-cell RNA-seq data: avoiding problems with zero counts}
\author{Aaron Lun and Karsten Bach}

\begin{document}
\maketitle

\section{Introduction}
Single-cell RNA sequencing (scRNA-seq) is a powerful technique that allows researchers to characterize the gene expression profile of single cells.
From each cell, mRNA is isolated, reverse-transcribed and subjected to massively parallel sequencing \cite{stegle2015computational}.
The sequencing reads are then mapped to a reference genome, whereby the number of reads mapped to each gene can be used to quantify the expression of that gene.
Alternatively, transcript molecules can be counted directly using unique molecular identifiers (UMIs) \cite{islam2014quantitative}.
Count data can be analyzed to identify new cell subtypes and to detect highly variable or differentially expressed (DE) genes between cell subpopulations.
This type of single-cell resolution is not possible with bulk RNA sequencing of cellular populations.
However, the downside is that the counts often contain high levels of technical noise with many ``drop-outs'', i.e., zero or near-zero values.
This is due to the difficulties in sequencing low amounts of RNA per cell, which decreases the capture efficiency during library preparation.
Moreover, the capture efficiency often varies from cell to cell, such that counts cannot be directly compared between cells.

Normalization of the scRNA-seq counts is a critical step that corrects for differences in capture efficiency between cells.
Two broad classes of normalization approaches are available -- those using spike-in RNA sets, and those using the counts for cellular RNA.
In the former, the same quantity of spike-in RNA is added to each cell prior to library preparation \cite{stegle2015computational}.
Any difference in the coverage of the spike-in transcripts must be caused by differences in capture efficiency between cells.
Normalization is then performed by scaling the counts to equalize spike-in coverage between cells.
For the methods using the cellular counts, the assumption is that most genes are not DE across the sampled cells.
Counts are scaled so that there is, on average, no fold-difference in expression between cells for the majority of genes.
This is the underlying concept of commonly used methods such as DESeq \cite{anders2010differential} and trimmed mean of M-values (TMM) normalization \cite{robinson2010scaling}.
An even simpler approach involves scaling the counts to remove differences in library sizes between cells.

The type of normalization that can be used often depends on the characteristics of the data set.
In some data sets, spike-in data may not be present -- for example, droplet-based protocols \cite{klein2015droplet,macosko2015highly} do not allow spike-ins to be easily incorporated.
This obviously precludes the use of spike-in normalization.
The methods based on cellular counts can be applied more generally but have their own deficiencies.
Normalization by library size is insufficient when DE genes are present, as composition biases can introduce spurious differences between cells \cite{robinson2010scaling}.
DESeq or TMM normalization are more robust to DE but rely on the calculation of ratios of counts between cells.
This is not straightforward in scRNA-seq data, where the high frequency of drop-out events interferes with stable normalization.
A large number of zeroes will result in nonsensical size factors from DESeq or undefined M-values from TMM.
One could proceed by removing the offending genes during normalization for each cell, but this may introduce biases if the number of zeroes varies across cells.

Correct normalization of scRNA-seq data is essential as it determines the validity of all downstream quantitative analyses.
In this article, we describe a deconvolution approach that improves the accuracy of non-DE normalization without using spike-ins.
Briefly, normalization is performed on pooled counts for multiple cells, where the incidence of problematic zeroes is reducedby summing across cells.
The pooled size factors are then deconvolved back to those for the individual cells.
Using a variety of simple simulations, we demonstrate that our approach outperforms the direct application of existing normalization methods for count data with many zeroes.
We also show a similar difference in behaviour on several real data sets, where the use of different normalization methods affects the final biological conclusions.
These results suggest that our approach is a viable alternative to existing methods for general normalization of scRNA-seq data.

\section{Existing normalization methods fail with zero counts}

\subsection{The origin of zero counts in scRNA-seq data}
The high frequency of zeroes in scRNA-seq data is driven by both biological and technical aspects.
Gene expression is highly variable across cells due to cell-to-cell heterogeneity and phenomena like transcriptional bursting \cite{marinov2014singlecell}.
Such variability is likely to result in zero counts for the lowly expressed genes.
It is also technically difficult to process low quantities of input RNA into sequenceable libraries.
This results in high dropout rates whereby low-abundance transcripts are not captured during library preparation \cite{brennecke2013accounting}.

At this point, it is important to distinguish between stochastic and systematic zeroes.
Systematic zeroes refer to genes that are constitutively silent in a cell (sub)population, such that the count must be zero for all cells in that population.
These are generally not problematic as they contain no information and can be removed prior to normalization.
Stochastic zeroes are found in genes that are actively expressed but obtain counts of zero in some cells due to sampling stochasticity.
These genes may contain information about the relative differences between cells, so removing them prior to normalization may introduce biases.

\subsection{A brief description of existing non-spike-in methods}
Here, we only consider those normalization methods not based on spike-in data.
This is motivated by the desire to obtain a general method that can be applied in all data sets.
In particular, we will review three approaches that are commonly used for RNA-seq data -- DESeq, TMM and library size normalization.

DESeq normalization was originally introduced as part of the DESeq package for detecting DE genes \cite{anders2010differential}.
It first constructs an ``average'' reference library, in which the ``count'' for each gene is defined as the geometric mean of the counts for that gene across all real libraries.
Each real library is then normalized against this average.
Specifically, for each gene, it computes the ratio of the count in each library to that in the average library.
The size factor for each library is defined as the median of this ratio across all genes.
This value represents the extent to which the counts in that library should be downscaled, 
    in order to eliminate any systematic differences in expression between libraries for the majority of (assumed) non-DE genes.

TMM normalization was introduced as part of the edgeR package for DE testing \cite{robinson2010edgeR}.
This method selects one library to be a reference, and normalizes each other library against the reference.
Specifically, for each library, M-values (i.e., library size-adjusted log$_2$-ratios in expression) are computed against the reference for all genes.
The genes with the most extreme M-values are trimmed away.
High- or low-abundance genes are similarly removed.
The weighted mean of the remaining M-values is computed, where the weighting is performed according to the asymptotic variance of each M-value.
This is used to define the normalization factor for each library as $2^x$, where $x$ is the weighted mean for that library.
The normalization factor represents the downscaling required to eliminate systematic differences between libraries, additional to that required to equalize the library sizes.
Taking the product of the normalization factor and library size for each library (i.e., the effective library size) yields a value that is functionally equivalent to the size factor.

Both DESeq and TMM normalization assume that most genes are not DE between libraries.
Any systematic difference in expression across the majority of genes is treated as bias.
This is duly incorporated into the size/normalization factors and removed upon scaling.
If the non-DE assumption does not hold, the computed factors will not be accurate.
In addition, both methods perform poorly in the presence of a large number of zeroes.
For DESeq normalization, the geometric mean will be equal to zero for genes with a zero count in any library, such that the ratios for that gene become undefined.
Conversely, a library with zero counts for a majority of genes will have a size factor of zero, which precludes any sensible scaling.
For TMM normalization, M-values are undefined when the count in either library is zero.
In such conditions, both methods typically require \textit{ad hoc} workarounds such as the removal of zero counts within each library.

Finally, library size normalization is another commonly used approach for normalizing RNA-seq data.
This involves scaling the counts such that the library size is the same across libraries,
    and is the basis for measures of normalized expression like counts- or transcripts-per-million.
While simple, this approach is not robust in the presence of DE genes \cite{robinson2010scaling}.
One can imagine a scenario where a single gene is strongly upregulated in one library compared to another.
This increases the size of the first library, as more reads are generated from the upregulated gene.
Normalization by library size will then scale up the counts for the second library to compensate.
This introduces spurious differences in the counts between libraries for the non-DE genes.
The likely presence of DE in real data means that library size normalization is often inappropriate.

\subsection{Performance of existing methods on simulated data with zeroes}
To test the performance of existing methods, simulated scRNA-seq data was generated with DE genes and a large number of stochastic zeros.
Consider a cell $j$ in a subpopulation $s$.
This subpopulation may represent cell type or some other biological condition, e.g., drug treatment. 
For each gene $i$ in this cell, the count $y_{ij}$ was sampled from a negative binomial (NB) distribution with mean $\theta_{j}\lambda_{is}$.
The $\theta_{j}$ term represents cell-specific biases (e.g., in capture efficiency) that must be normalized out, 
    and is sampled for each cell such that 
\[
\log_2(\theta_j) \sim \mathcal{N}(0, 0.25) \;.
\]
The $\lambda_{is}$ term denotes the expected number of transcripts of gene $i$ for cells in subpopulation $s$.
It is defined as $\lambda_{is}=\phi_{is}\lambda_{i0}$ where $\phi_{is}$ represents the DE for this gene in this subpopulation, and 
\[
\log_2(\lambda_{i0}) \sim \mbox{Uniform}(3, 6) \;.
\]
This recapitulates the spread of abundances in real data.
The NB dispersion is defined for each gene as 
\[
    \varphi_i = 2 + \frac{100}{\lambda_{i0}}
\]
to represent a decreasing mean-dispersion trend.
Large values for $\varphi_i$ are consistent with the high levels of technical noise and biological heterogeneity in scRNA-seq studies,
    and ensure that a large number of stochastic zeroes are generated.
Approximately 30-50\% of all counts are zero in each simulated library.

The simulation design involves 10000 genes for three subpopulations of 250 cells each.
For each subpopulation, a unique set of $G$ genes was randomly chosen.
DE was introduced by setting $\phi_{is}$ to some constant $\phi_s$ for $p_s$ of the $G$ genes (i.e., upregulated) and to $\phi_s^{-1}$ for the rest (downregulated).
The value of $\phi_s$ was set to 5 for all $s$, while $p_s$ was set to 20, 50 and 80\% for the first, second and third subpopulations, respectively.
This provides each subpopulation with a unique expression signature containing different numbers of DE genes in each direction.
Simulations were performed for $G = 0$ (no DE), 1000 (moderate DE) and 3000 (strong DE).
The simulation was also repeated with $G$ set to 3000, $p_s$ set to 50\% for all $s$ and $\phi_{s}$ set to 2, 5 and 10 for the first, second and third subpopulations, respectively.
This represents an alternative scenario where the number of DE genes in either direction is the same but the magnitude of DE is different between subpopulations.
For all scenarios, simulated data was generated by sampling NB-distributed counts as described.

DESeq, TMM and library size normalization were applied to this data.
For DESeq normalization, the geometric mean was computed by replacing all zero counts with unity.
This is necessary to avoid a situation where a majority of genes have geometric means of zero, such that the majority of ratios to the geometric mean would be undefined.
Zero counts were also removed within each library prior to calculation of the median ratio, to avoid obtaining size factors equal to zero.
For TMM normalization, the calcNormFactors function in the edgeR package (v3.12.0) was used with default settings.
All undefined M-values were automatically removed prior to trimming and calculation of the normalization factor for each library.
The corresponding size factor was defined as the effective library size, i.e., the product of the library size and the normalization factor for each library.
For library size normalization, the total library size was used directly as the size factor for each cell.
The true size factor for each cell is $\theta_j$, as it represents the extent of scaling required to remove the cell-specific bias of $j$.
Estimated size factors were then compared to the true values for all cells.

All methods yield size factors that systematically deviate from the true values (Figure~\ref{fig:existing_sim}).
For DESeq and TMM normalization, large size factors are consistently underestimated while small size factors are overestimated.
This is a consequence of removing stochastic zeroes prior to normalization.
Cells with low $\theta_j$ are likely to contain more stochastic zeroes, as the mean of the sampling distribution for the counts is lower.
If these zeroes are removed prior to DESeq normalization, the median ratio will be computed from the remaining non-zero counts.
This shifts the median upwards and results in overestimation.
Similarly, the distribution of M-values will be shifted towards positive values upon removal of zeroes.
This is because stochastic zeroes represent sampled values below some non-zero mean, and would generally correspond to negative M-values.
Their removal increases the (trimmed) mean of M-values and biases the estimate of the TMM normalization factor.
The converse applies to cells with large $\theta_j$.
Recall that size factors have a relative interpretation across cells, so overestimation of small $\theta_j$ will lead to a concomitant underestimation for large $\theta_j$.

\begin{figure}[tbp]
\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm  5mm,clip]{results/size_1.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/size_2.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/size_3.pdf}
\includegraphics[width=\textwidth,trim=0mm  5mm 2mm 20mm,clip]{results/size_4.pdf}
\subcaption{}\label{subfig:size_sim}
\end{minipage}
\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm  5mm,clip]{results/TMM_1.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/TMM_2.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/TMM_3.pdf}
\includegraphics[width=\textwidth,trim=0mm  5mm 2mm 20mm,clip]{results/TMM_4.pdf}
\subcaption{}\label{subfig:tmm_sim}
\end{minipage}
\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm  5mm,clip]{results/lib_1.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/lib_2.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/lib_3.pdf}
\includegraphics[width=\textwidth,trim=0mm  5mm 2mm 20mm,clip]{results/lib_4.pdf}
\subcaption{}\label{subfig:lib_sim}
\end{minipage}
\caption{
    Performance of existing normalization methods on the simulated data with DE genes and stochastic zeroes.
    The size factor estimate for each cell is plotted against its true values for (\subref{subfig:size_sim}) DESeq,
        (\subref{subfig:tmm_sim}) TMM and (\subref{subfig:lib_sim}) library size normalization.
    Simulations were performed with no DE (first row), moderate DE (second row), strong DE (third row) and varying magnitudes of DE (fourth row).
    Axes are shown on a log-scale.
    For comparison, each set of size factors was scaled such that the same grand mean across cells was the same as that for the true values.
    The red line represents equality between the rescaled estimates and true factors.
    Cells in the first, second and third subpopulations are shown in black, blue and orange, respectively.
}
\label{fig:existing_sim}
\end{figure}

The presence of DE genes results in a further deterioration in performance of all methods (Figure~\ref{fig:existing_sim}).
The divergence between the true and estimated size factors increases as the number of DE genes increases,
    consistent with a decrease in the validity of the non-DE assumption required by all methods.
To illustrate, consider the strong DE simulation.
Across the three subpopulations, there are 9000 genes involved in subpopulation-specific signatures, i.e., 90\% of all genes exhibit DE within this data set.
An assumption of a non-DE majority of genes is clearly invalid in this scenario.
Any systematic differences in expression between cells of different subpopulations will now include genuine DE in addition to cell-specific biases.
Subsequently, the magnitude of DE will be incorporated in the size factors computed from each method, such that they are no longer accurate estimates of $\theta_j$.
The estimates also split into three distinct groups in Figure~\ref{fig:existing_sim}, each corresponding to a different subpopulation.
Size factors tend to be overestimated for subpopulations with more DE, as the increased expression of upregulated genes drags up the median ratio/trimmed mean.

It is worth noting that library size normalization is not affected by stochastic zeroes.
This is because the library size is stably computed by summing across all genes in each cell.
This results in improved performance in the simple simulation with no DE (Figure~\ref{subfig:lib_sim}).
However, as previously discussed, the use of the library size as a size factor assumes that all genes are non-DE in each cell.
Violations of this assumption lead to substantial estimation errors, which can be seen in the results for the simulations involving any DE.

\section{Improving normalization accuracy with deconvolution}

\subsection{Overview of the deconvolution strategy}
The aim of the deconvolution strategy is to normalize on summed expression values from pools of cells.
Summation across cells results in fewer zeroes, which means that the ensuing normalization is less susceptible to the errors observed for existing methods.
While normalization accuracy is improved, the estimated size factors are only relevant to the pools of cells.
This is not particularly interesting for downstream analyses focusing on single cells.
In order to be useful, the size factor for each pool is deconvolved back into the size factors for its constituent cells.
This ensures that cell-specific biases can be properly normalized.

\subsection{Summation and deconvolution with linear equations}
Define the size-adjusted expression of gene $i$ as $\pi_{ij} = y_{ij}T_j^{-1}$, where $T_j$ is the total library size for cell $j$.
The expectation of $\pi_{ij}$ can then be written as $\theta_j\lambda_{is} T_j^{-1}$.
Further assume that we have an arbitrary set of cells $\mathcal{S}_k$.
Denote the sum of $\pi_{ij}$ across $\mathcal{S}_k$ as $s_{ik}$ for gene $i$.
The values of $s_{ik}$ across all genes constitute an overall expression profile for the pool of cells corresponding to $\mathcal{S}_k$.
The expectation of $s_{ik}$ is equal to 
\[
    E(s_{ik}) = \lambda_{is} \sum_{j \in \mathcal{S}_k} \theta_j T_j^{-1}\;.
\]
Define $u_{i}$ as the mean of expression values for gene $i$ across all $N$ cells in the entire data set.
The values of $u_{i}$ across all genes represent the expression profile for an averaged reference pseudo-cell.
The cell pool for set $\mathcal{S}_k$ is then normalized against this reference pseudo-cell.
Let $r_{ik}$ denote the ratio of $s_{ik}$ to $u_{i}$ for a non-DE gene $i$.
The expectation of $r_{ik}$ represents the size factor for the pooled cells in $\mathcal{S}_k$, and is written as
\begin{equation}
    E(r_{ik}) \approx \frac{E(s_{ik})}{E(u_{i})} 
    = \frac{\sum_{\mathcal{S}_k} \theta_j T_j^{-1}}{ N^{-1} \sum_{\mathcal{S}_0} \theta_j T_j^{-1}} 
    = \frac{\sum_{\mathcal{S}_k} \theta_j T_j^{-1}}{C}
    \label{eqn:linear_single}
\end{equation}
where $C$ is a constant that does not depend on the gene, cell or set $\mathcal{S}_k$.
The above approximation is valid if the variance of the sums is low -- 
    this should be the case for large sets, as the variability relative to the mean should decrease due to the law of large numbers.
$E(r_{ik})$ is estimated by taking any robust average (in this case, the median) of $r_{ik}$ across $i$.
Robustness protects the average against DE genes with extreme ratios.

The estimated value of $E(r_{ik})$ for the set can be used to obtain estimates for $\theta_j$ for each cell.
A linear equation is set up based on the expression in Equation~\ref{eqn:linear_single}, 
by replacing $E(r_{ik})$ with its estimate and treating the $\theta_j T_j^{-1}$ as unknown parameters to be estimated.
The constant $C$ can be set to unity and ignored, as it does not contribute to the relative differences between size factors.
Repeating the estimation of $E(r_{ik})$ for different sets of cells in $\mathcal{S}_{ik}$ will generate an overdetermined system of linear equations, 
    in which the $\theta_j T_j^{-1}$ for each $j$ is represented at least once.
This system can then be solved with least-squares methods to obtain estimates of $\theta_j T_j^{-1}$ for all cells.
Multiplication by $T_j$ for each cell will then yield an estimate of $\theta_j$.

This approach may seem somewhat circuitous, given that $\theta_j$ could be estimated directly from the counts for each individual cell.
However, summation reduces the number of stochastic zeroes that cause problems in existing methods.
As a result, ratios computed from pooled expression profiles are more accurate.
This improvement will propagate back to the $\theta_j$ estimates for each $j$ when the linear system is solved.

\subsection{Obtaining sensible least-squares solutions}
The linear system can be solved using standard methods, such as those based on the QR decomposition.
However, with such methods, it is theoretically possible to obtain negative estimates for $\theta_j$.
Such values are obviously nonsensical as counts cannot be scaled to negative values.
One situation in which this might occur involves data with a large spread of $\theta_jT_j^{-1}$ values, 
    such that the true value of $\theta_jT_j^{-1}$ is already close to zero for some cells.
Errors in estimation may then be sufficient to push the estimated $\theta_j$ below zero.
Protection is provided by using linear inverse models in the limSolve package (v1.5.5.1, \url{https://cran.r-project.org/web/packages/limSolve/index.html}) to constrain the estimates to non-negative values.
This will not provide sensible size factor estimates for the offending cells -- these are set to zero and should be removed --
    but will ensure that the estimates for other cells are not distorted by negative values elsewhere in the system.

Note that counts could be summed rather than size-adjusted expression values.
This means that the solution of the linear system will directly yield estimates of $\theta_j$.
However, we use size-adjusted values to ensure that the sum is not dominated by a small number of very large libraries.
Information from each cell will then be weighted equally when computing the median ratio for each set, regardless of library size.
It also reduces the risk of obtaining negative estimates for very small libraries.
Such libraries have very small $\theta_j$ and would be unduly influenced by (relatively) large estimation errors for cells with larger libraries.

\subsection{Clustering to weaken the non-DE assumption}
This approach makes some moderately strong assumptions regarding the nature of DE across the data set.
The use of the median is only valid when less than 50\% of genes are DE in any direction in the cell pool compared to the reference pseudo-cell,
    i.e., less than 50\% of genes can be upregulated and less than 50\% of genes can be downregulated.
If more DE genes are present, the median will not represent a robust average across non-DE genes.
This condition generally requires a proportion of genes to be constantly expressed across all cells in the data set 
    -- otherwise, all genes would be DE against the average in at least one pseudo-cell -- 
    and is only guaranteed to be true when that proportion is equal to or greater than 50\% of all genes.
This is similar to that required for DESeq normalization where an average reference is also used.

To reduce the strength of the non-DE assumption, cells can be clustered based on their expression profiles.
The deconvolution method is then applied to the cells in each cluster $\mathcal{C}$ separately,
    where the sets $\mathcal{S}_k$ are nested within each $\mathcal{C}$.
This normalizes each cell pool of $\mathcal{S}_k$ to a cluster-specific reference pseudo-cell for $\mathcal{C}$,
    yielding a cluster-specific size factor of $f_{j}'$ for cell $j \in \mathcal{C}$ after deconvolution.
These cluster-specific size factors must be rescaled before they can be compared between clusters.
To do so, the reference pseudo-cells for all clusters are normalized against each other.
This is done by selecting a single ``baseline'' pseudo-cell against which all other pseudo-cells are normalized.
The median ratio $\tau_{\mathcal{C}}$ of the expression values is computed for the pseudo-cell of each cluster against the baseline pseudo-cell
    (obviously, the cluster chosen as the baseline will have $\tau_{\mathcal{C}}=1$).
The overall size factor for cell $j$ in cluster $c$ is subsequently defined as $f_j = f_{j}'\tau_{\mathcal{C}}$.

% This is justified by considering normalization factors; you multiply the normalization factor within clusters with that between clusters to get the overall factor 
% (independent of library size), then you multiply by the library size to get the size factors. 
% The same applies here, except that you multiply by library size first to get the within-cluster size factors, then you multiply between clusters.
% Or, in other words, the within-cluster size factors capture library size differences and additional within-cluster differences;
% you only need to multiply by additional between-cluster differences to get the overall size factors 
% (i.e., you don't need to compute the size factors between clusters, only the normalization factors).

The use of within-cluster normalization reduces the number of DE genes, as all cells in each cluster have similar expression profiles.
This avoids inaccurate estimation of the size factors due to violations of the non-DE assumption.
Moreover, the pseudo-cells are normalized in pairwise comparisons to a baseline.
This weakens the assumption as a non-DE majority is only required across pairs of pseudo-cells/clusters, rather than across the entire data set.
For example, in the simulation with varying magnitude of DE, only 60\% of genes are DE between any two subpopulations, while 90\% of genes exhibit DE across all subpopulations.

Any clustering technique can be used to group cells with similar expression profiles.
We favour a correlation-based approach using $1-\rho_{xy}$ as the distance, where $\rho_{xy}$ denotes Spearman's rank correlation coefficient between the counts of cells $x$ and $y$.
In this manner, a distance matrix is constructed between all pairs of cells.
Hierarchical clustering is then performed on this matrix using Ward's clustering criterion.
A dynamic tree cut is used to define clusters of cells using the dynamicTreeCut package v1.62 (\url{https://cran.r-project.org/web/packages/dynamicTreeCut/index.html}).
This is done so that each cluster contains a minimum number of cells (200 by default) required for stable deconvolution.
Correlation-based methods are attractive as they are insensitive to global scaling of the expression values in each cell.
Prior normalization is not required, which avoids a circular dependence between normalization and clustering.

By default, the baseline pseudo-cell is chosen from the cluster where the mean library size per cell is equal to the median of the mean per cell for all clusters
    (or, for an even number of clusters, the cluster with the smallest mean library size above the median).
This uses the mean library size as a rough proxy for cell similarity.
The baseline cluster is likely to be least dissimilar to every other cluster, which reduces the amount of DE genes during pairwise normalization.
More intelligent choices of the baseline can be used if the similarities between clusters are known, e.g., from visualization after dimensionality reduction.

Systematic zeroes within each cluster are also removed prior to deconvolution in that cluster.
This removes genes that have only zero counts across all cells in the cluster.
Such genes provide no information for normalizing between cells in the same cluster, and their removal will not affect the cluster-specific size factor estimates.
However, these genes are retained during rescaling of the size factors between clusters.
This is because they will have non-zero counts in at least one cluster (assuming that systematic zeroes across the entire data set have already been removed).
Removal of such genes will distort the median ratio between pseudo-cells and lead to biased size factor estimates, as described previously for the existing methods.

\subsection{Selecting cell pools to sum}
The pool of cells in each set $\mathcal{S}_{ik}$ consists of cells with similar library sizes.
Cells in a given cluster are ordered by their total counts and partitioned into two groups, depending on whether the ranking of each cell is odd or even.
These cells are arranged in a ring, with odd cells on the left and even cells on the right.
Conceptually, one can start at the 12 o'clock position on the ring, for the largest libraries; moving clockwise through the even cells with decreasing library size;
reaching the smallest libraries at 6 o'clock; and then, continuing to move clockwise through the odd cells with increasing library size (Figure~\ref{fig:library_ring}).
For summation, a sliding window is moved cell-by-cell across this ring where each window contains the same number of cells.
These cells are used to define a single instance of $\mathcal{S}_{k}$.
Thus, each window defines a separate equation in the linear system.
The use of a ring means that the window is still defined at the smallest and largest libraries.
In contrast, sliding a window across a linear ordering of cells will result in truncated windows at the boundaries.

\begin{figure}[bt]
    \begin{center}
        \includegraphics[width=0.5\textwidth]{pics/library_ring.pdf}
    \end{center}
    \caption{
        Ring arrangement of cells ordered by library size.
        Each circle represents a cell where the size of the circle corresponds to the library size of that cell.
        Even- and odd-ranking cells lie on opposite sides, with the largest and smallest libraries at the top and bottom, respectively.
        Cells lying in a window of length 4 are highlighted in red.
        Different instances of the window are obtained by sliding the window across the ring.
    }
    \label{fig:library_ring}
\end{figure}

The pooling of cells with similar library sizes is designed to provide some robustness to DE.
Specifically, the library size is used as a rough proxy for cell similarity.
This is motivated by the fact that different cell types tend to have systematic differences in the library sizes, 
    e.g., due to type-specific differences in total RNA or capture efficiency.
Summation of cells with similar library sizes (and by implication, similar expression profiles) reduces the number of DE genes in the cell pool relative to the pseudo-cell.
This further weakens the non-DE assumption that is required by the deconvolution approach.
Note that this pooling strategy is performed on cells within a single cluster at a time.
It complements the use of clustering by protecting against any residual DE within each cluster.
This is especially true for heterogeneous populations that are difficult to partition, or in cases where larger clusters must be formed due to low numbers of cells.

% In small samples, it mightn't matter whether or not the cells have similar expression profiles;
% DE anywhere in the data set will contaminate the average reference anyway, so the amount of DE in each cell pool relative to the pseudo-cell would be the same.
% However, if the cluster is large enough, minor DE will average out in the pseudo-cell.
% Then we definitely can see an advantage from summing similar cells at a time in each cell pool. 

The total number of equations in the linear system is equal to the number of cells.
The $\theta_jT_j^{-1}$ term for each cell is represented in $w$ equations, where $w$ denotes the size of the window.
By using different values of $w$, additional equations can be added to improve the precision of the estimates. 
Specifically, values of $w$ are set to 20, 40, 60, 80 and 100.
These are large enough to obtain stable sums yet small enough to maintain resolution, i.e., by ensuring that cells with very different library sizes are not summed together.
This increases the total number of equations in the system and means that each $\theta_j$ is represented in 300 equations. 

An additional set of equations is added to ensure that the system is solvable.
In each equation, the $\theta_jT_j^{-1}$ for each cell is equated to the size factor estimated directly from its single-cell counts.
These equations are assigned very low weights compared to the other equations involving summed cells, with weights of $10^{-6}$ and unity respectively.
A weighted least-squares approach is then applied to solve the linear system.
In the matrix containing the coefficients of the linear system, the incorporation of the additional equations ensures that the columns are linearly independent.
This ensures that a single solution can be obtained.
Due to their low weights, the additional equations will not contribute substantially to the weighted least-squares solution.
This means that the estimated values are driven primarily by the equations for the summed cells.

\subsection{Performance of the deconvolution approach on simulated data}
The deconvolution method provides accurate estimates of the size factor estimates in most simulation scenarios (Figure~\ref{fig:sim_cluster_DE}).
This is consistent with the reduced number of zero counts in the summed counts for each set of cells.
The median ratio for each set is more accurately computed, which improves the accuracy of the size factor estimates for the individual cells upon deconvolution.
Systematic under- or overestimation of the size factors for cells with large or small $\theta_j$ is avoided.
Some inaccuracy is observed in Figure~\ref{subfig:sumclust_3}, where the non-DE assumption is partially violated by large numbers of DE genes between subpopulations.
However, the deconvolution method is still more accurate than the existing methods in the third row of Figure~\ref{fig:existing_sim}
    -- unlike DESeq or TMM normalization, the estimates here are proportional to the true values within each subpopulation, 
       and the deviation from the diagonal is smaller than that for library size normalization.

\begin{figure}[btp]
\begin{center}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_1.pdf}
    \subcaption{}\label{subfig:sumclust_1}
\end{minipage}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_2.pdf}
    \subcaption{}\label{subfig:sumclust_2}
\end{minipage}  \\ 
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_3.pdf}
    \subcaption{}\label{subfig:sumclust_3}
\end{minipage}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_4.pdf}
    \subcaption{}\label{subfig:sumclust_4}
\end{minipage}
\end{center}
    \caption{
        Size factor estimates from the deconvolution method in the simulation with DE genes and stochastic zeroes.
        Results are shown for scenarios with (\subref{subfig:sumclust_1}) no DE, (\subref{subfig:sumclust_2}) moderate DE, 
                (\subref{subfig:sumclust_3}) strong DE and (\subref{subfig:sumclust_4}) varying magnitude of DE.
        Cells in the first, second and third subpopulations are shown in black, blue and orange, respectively.
        Axes are shown on a log-scale, and the red line represents equality with the true factors.
    }
    \label{fig:sim_cluster_DE}
\end{figure}

One potential criticism of this approach is that it relies on clustering information.
Cluster-specific normalization might introduce artificial differences between cells in different clusters, 
    such that the statistical rigour of downstream analyses (e.g., to detect DE between clusters) would be compromised.
Such problems are avoided by normalization between the cluster-specific reference pseudo-cells to ensure that cells are comparable between clusters.
The end result is that all systematic differences between cells are removed.
This is equivalent to the outcome of a hypothetical one-step method that does not use cluster information (and is robust to DE and stochastic zeroes, unlike existing methods).
Moreover, normalization accuracy in Figure~\ref{fig:sim_cluster_DE} is unaffected by the use of clustering prior to deconvolution, which suggests that this approach is valid.

\section{Differences in normalization are recapitulated in real data}

\subsection{Overview of the data sets}
We also examined the behaviour of the deconvolution method in real scRNA-seq data.
The first data set was taken from a study of the somatosensory cortex and hippocampal region of the mouse brain \cite{zeisel2015brain}.
Libraries were prepared for over 3000 brain-derived single cells using the Fluidigm C1 system.
Gene expression was quantified for each cell by counting UMIs after sequencing.
Counts for all cells were obtained from \url{http://linnarssonlab.org/cortex}.
The second data set was generated using the inDrop protocol on mouse embryonic stem cells \cite{klein2015droplet}.
Libraries were prepared for over 10000 cells, and quantification was performed with UMIs.
Counts were obtained from the NCBI Gene Expression Omnibus with the accession GSE65525.

For both data sets, low-abundance genes were defined as those with an average count below 0.2 across all cells.
These were considered to be systematic zeroes (with some non-zero counts due to residual transcription, mapping errors, etc.) and removed prior to further analysis.
The set of External RNA Controls Consortium (ERCC) spike-ins in the brain data set were also removed.
This ensures that normalization is only performed using the counts for the cellular genes.
For the inDrop data set, counts were only used for cells before withdrawal of leukemia inhibitory factor (LIF), and those 7 days after withdrawal.
This resulted in a final set consisting of approximately 1700 cells.
Cells in the intervening time points were not considered as the library sizes were too small -- the average total count across all genes was around 3000 for each cell.

\subsection{Deconvolution yields a wider spread of size factors}
Each normalization method was applied on the counts for all cells in both data sets.
Compared to DESeq and TMM normalization, the deconvolution method yields a wider spread of size factor estimates (Figure~\ref{fig:real_spread}).
This is consistent with the simulation results where the stochastic zeroes cause the estimates to be biased towards unity for the existing methods.
Indeed, approximately 60\% of counts are equal to zero in each cell for both data sets, even after the removal of low-abundance genes.
This indicates that the presence of stochastic zeroes is an intrinsic property of high-throughput scRNA-seq data and cannot simply be ignored.
The deconvolution method reduces the number of zero counts by summing across cells.
This avoids the bias towards unity in the simulations and increases the range of the estimates in the brain and inDrop data.

\begin{figure}[btp]
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 0mm 0mm 5mm,clip]{realdata/Zeisel_NormFactors.pdf}
        \subcaption{}\label{subfig:spread_brain}
    \end{minipage}
    \begin{minipage}{0.48\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 0mm 0mm 5mm,clip]{realdata/Klein_NormFactors.pdf}
        \subcaption{}\label{subfig:spread_indrop}
    \end{minipage}
    \caption{
        Boxplots of the estimated size factors across all cells in the (\subref{subfig:spread_brain}) brain or (\subref{subfig:spread_indrop}) inDrop data,
            for all normalization methods.
        Axes are on a log-scale, and dots represent outlier cells with extreme size factors.
        To simplify comparisons between methods, each set of size factors was scaled to have a median of unity.
    }
    \label{fig:real_spread}   
\end{figure}

The size factor estimate from deconvolution is a monotonic increasing function of that from the other methods (Figure~\ref{fig:real_comp}).
This suggests that the methods are qualitatively similar, in that the rankings of size factors are in rough agreement across methods.
However, the quantitative behaviour of each method is substantially different.
Deconvolution yields a wider range of estimates compared to DESeq and TMM normalization, consistent with the simulation results.
Differences are also present between the estimates from deconvolution and those from library size normalization in the brain data (Figure~\ref{subfig:comp_lib}).
This is attributable to the likely presence of DE genes between cell types, which will confound library size normalization.
In contrast, deconvolution uses a median-based approach that is robust to extreme ratios caused by DE.
The two methods are more similar for the inDrop data where less DE is expected within the same cell type.

\begin{figure}[btp]
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_SFvDeconv.pdf}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Klein_SFvDeconv.pdf}
        \subcaption{}\label{subfig:comp_sf}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_TMMvDeconv.pdf}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Klein_TMMvDeconv.pdf}
    \subcaption{}\label{subfig:comp_tmm}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_LibvDeconv.pdf}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Klein_LibvDeconv.pdf}
        \subcaption{}\label{subfig:comp_lib}
    \end{minipage}
    \caption{
        Comparisons between the estimated size factors from the deconvolution method to those from (\subref{subfig:comp_sf}) DESeq,  (\subref{subfig:comp_tmm}) TMM
            or (\subref{subfig:comp_lib}) library size normalization.
        This is shown for the brain (top) and inDrop data sets (bottom).
        Axes are on a log-scale, and the blue line represents equality between the two sets of factors.
    }
    \label{fig:real_comp}  
\end{figure}

\subsection{Different normalization schemes change the DE profile}
To gauge the impact of the differences in the normalization methods, a DE analysis was performed with edgeR (v3.12.0) on both data sets with the different size factor estimates.
For the brain data set, the count data was subsetted to contain only cells classified as microglia or oligodendrocytes \cite{zeisel2015brain}.
Normalization factors were computed by dividing each size factor by the corresponding library size for each cell.
The estimateDisp function was used to estimate a gene-specific NB dispersion for each gene, by shrinking each estimate towards a mean-dispersion trend \cite{chen2014differential}.
A generalized linear model was fitted for each gene using a one-way layout with the two cell types \cite{mccarthy2012differential}.
A likelihood ratio test was performed to test each gene for DE between cell types.
The Benjamini-Hochberg correction was applied to the $p$-values to control the false dicovery rate (FDR) at a threshold of 5\%.
This process was repeated for the inDrop data to test for DE after LIF withdrawal.

Substantial differences are present in the numbers of DE genes between methods (Table~\ref{tab:real_de}).
In general, deconvolution results in fewer DE genes than DESeq and TMM normalization.
The numbers of DE genes are also more balanced between up- and downregulated genes.
The total number of DE genes from library size normalization are more similar to those from deconvolution, 
    but the numbers of DE genes changing in each direction is still different.
The set of DE genes detected in either direction by deconvolution also tends to be a superset or subset of that detected by the existing methods.
This is consistent with a difference in overall scaling of the average expression values between the different biological conditions, 
    such that the DE log-fold changes from deconvolution are consistently shifted relative to those from the existing methods.

\begin{table}[bt]
\caption{
    Number of DE genes detected by edgeR at a FDR of 5\% in each data set, using the size factor estimates from different methods for normalization.
    This is also separated into the number of up- and downregulated genes for each data set.
    Upregulation refers to increased expression in oligodendrocytes over microglia in the brain data, and to increased expression after LIF withdrawal in the inDrop data.
}
\begin{center}
\begin{tabular}{l r r r c r r r}
\hline
\multirow{2}{*}{\textit{Method}} & \multicolumn{3}{c}{\textit{Brain}} && \multicolumn{3}{c}{\textit{inDrop}}  \\
\cline{2-4}
\cline{6-8}
& Total & Down & Up && Total & Down & Up \\
\hline
DEseq                               & 3910 & 630  & 3280 && 10557 & 7536 & 3021 \\
TMM                                 & 3848 & 688  & 3160 && 9670  & 5693 & 3977 \\
Library size                        & 3483 & 2000 & 1483 && 10140 & 4272 & 5868 \\
Deconvolution                       & 3494 & 1538 & 1956 && 9806  & 4825 & 4981 \\
$\quad$ shared with DESeq           & 2566 & 630  & 1936 && 7851  & 4825 & 3021 \\
$\quad$ shared with TMM             & 2644 & 688  & 1936 && 8805  & 4825 & 3977 \\
$\quad$ shared with library size    & 3019 & 1536 & 1483 && 9250  & 4269 & 4981 \\
\hline                                                   
\end{tabular}
\end{center}
\label{tab:real_de}
\end{table}

These results indicate that different normalization schemes will affect downstream steps like DE analyses.
For example, incorrect normalization may preserve (or even introduce) spurious differences in expression for non-DE genes by failing to properly correct for cell-specific biases.
Conversely, the log-fold changes for genuine DE genes may be distorted such that detection power is decreased.
For these data sets, over a thousand genes are found or lost when the deconvolution method is used instead of the existing methods.
This is likely to have some impact on the biological conclusions that can be taken from the DE analysis.

\subsection{Deconvolution and existing methods are inconsistent with spike-ins}
The brain data set also contains counts for ERCC spike-in genes that are commonly used for normalizing scRNA-seq data.
Briefly, a constant quantity of spike-in RNA is added to each cell and processed with the cellular RNA \cite{stegle2015computational}.
Upon sequencing and quantification, differences in the coverage of spike-in genes between cells are interpreted as cell-specific biases and removed.
At its simplest, spike-in normalization uses the total count across the spike-in genes as the size factor for each cell.
This is similar to library size normalization with the counts for the cellular genes, but is more robust as no DE is expected across cells for a constant spike-in set.
Problems with stochastic zeroes are avoided, and the assumption of a non-DE majority is not required.
However, spike-in normalization does assume that the same quantity of spike-in RNA can be added to each cell \cite{robinson2010scaling,marinov2014singlecell}.
It also assumes that the spike-ins behave similarly to the cellular transcripts \cite{grun2015design}.
Violations of these assumptions may compromise performance, as observed in some bulk RNA-seq data sets \cite{risso2014normalization}.

Examination of the data indicates that no correlation -- qualitative or quantitative -- 
    exists between the sets of size factors from spike-in and the other normalization methods (Figure~\ref{fig:real_spike}).
This is attributable to the differences in the underlying assumptions of each method.
If most genes are DE, deconvolution and the existing methods will be incorrect and spike-in normalization will be more accurate.
Conversely, spike-in normalization will be incorrect if spike-ins are not added precisely or if spike-in and cellular transcripts behave differently during library preparation.
These results suggest that the validity of each assumption requires some consideration before normalization.
For example, the similarity of the input cell types can be used to gauge the appropriateness of the non-DE assumption. 
Here, over half of the cells are classified as neuronal \cite{zeisel2015brain}, so a non-DE majority of genes is not entirely unreasonable.
Obviously, the truth of each assumption is largely unknown, so it is difficult to definitively determine the correct method for any given data set.

\begin{figure}[btp]
\begin{center}
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_ERCCvSF.pdf}
        \subcaption{}\label{subfig:spike_sf}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_ERCCvTMM.pdf}
        \subcaption{}\label{subfig:spike_tmm}
    \end{minipage} \\ 
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_ERCCvLib.pdf}
        \subcaption{}\label{subfig:spike_lib}
    \end{minipage} 
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_ERCCvDeconv.pdf}
        \subcaption{}\label{subfig:spike_sum}
    \end{minipage}
\end{center}
    \caption{
        Comparisons between the estimated size factors from spike-in normalization to those from (\subref{subfig:spike_sf}) DESeq, (\subref{subfig:spike_tmm}) TMM,
            (\subref{subfig:spike_lib}) library size normalization or (\subref{subfig:spike_sum}) the deconvolution method for all cells in the brain data set.                  
        All sets of size factors were scaled to have a median of unity.
        Axes are shown on a log-scale.
    }
    \label{fig:real_spike}  
\end{figure}

A more philosophical point is that spike-in normalization preserves differences in cell size and total RNA content.
In contrast, deconvolution and the existing methods will remove such differences, as they affect all genes within each cell and are absorbed into the cell-specific bias.
Whether or not this is appropriate depends on whether cell size differences are of interest.
In some scenarios, this may be the case such that spike-ins should be used, e.g., when studying cycling cells with differences in total RNA.
In other cases, cell size may be a confounding variable that needs to be removed, e.g., when focusing on changes in the expression profile between cells.
For these data sets, normalization should be performed with the deconvolution method.

\section{Conclusions}
Here, we have presented a normalization strategy for scRNA-seq data based on summation of expression values and deconvolution of pooled size factors.
This approach provides improved performance for size factor estimation, compared to existing methods on simulated data.
In particular, it avoids estimation inaccuracy in the presence of stochastic zeroes and is robust to DE in the data set.
Similar differences in the size factors across methods were also observed in analyses of real data,
    where the use of different size factor sets resulted in changes to the number and identity of detected DE genes.
This indicates that the choice of normalization method has a substantive impact on the results of downstream analyses.
Any increase in accuracy from our deconvolution approach is likely to have some beneficial effect on the validity of the biological conclusions.

The deconvolution strategy uses a clustering step to arrange similar cells into groups prior to normalization.
We have described the use of an empirical clustering scheme that uses the correlations in the observed expression profiles between cells.
However, any grouping of cells can be used as long as there are sufficient cells within each group.
Cells can be partitioned by known aspects of the experimental design, e.g., treatment condition, plate or chip of origin, batch.
For example, the inDrop data set could be partitioned into two groups corresponding to cells before and after LIF withdrawal.
If such information is available, empirical clustering may not be required.
This reduces computational work and avoids potential errors in grouping.

Another consideration during normalization is the choice between spike-in and other approaches.
Ideally, this should be driven by biology, i.e., whether cell size is relevant to the study.
In practice, though, issues such as cost and convenience determine whether spike-ins are added in the experiment.
This is especially true for the droplet-based protocols \cite{macosko2015highly,klein2015droplet} for which the consistent incorporation of spike-ins is not straightforward.
If spike-ins are not available, normalization must be performed with methods that assume a non-DE majority, e.g., TMM, deconvolution.
This allows for sensible removal of cell-specific biases without spike-in counts.

We have implemented our deconvolution approach as a R function, with C++ extensions for construction of the linear system.
It is available as part of the edgeR package in the Bioconductor project \cite{huber2015orchestrating}.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}

