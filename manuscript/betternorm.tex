\documentclass{article}
\usepackage[margin=2.5cm]{geometry}
\usepackage{amsmath}
\usepackage{subcaption}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{multirow}
\hypersetup{colorlinks=true,linkcolor=black,citecolor=black,urlcolor=black}
\urlstyle{same}

\title{Computing normalization factors for single-cell RNA-seq data: avoiding problems with zero counts}
\author{Aaron T. L. Lun and Karsten Bach}

\begin{document}
\maketitle

\newcommand{\suppdeconv}{1}
\newcommand{\suppspikesec}{2}
\newcommand{\suppring}{S1}
\newcommand{\suppspikefig}{S2}

\section{Introduction}
Single-cell RNA sequencing (scRNA-seq) is a powerful technique that allows researchers to characterize the gene expression profile of single cells.
From each cell, mRNA is isolated, reverse-transcribed and subjected to massively parallel sequencing \cite{stegle2015computational}.
The sequencing reads are mapped to a reference genome, whereby the number of reads mapped to each gene can be used to quantify the expression of that gene.
Alternatively, transcript molecules can be counted directly using unique molecular identifiers (UMIs) \cite{islam2014quantitative}.
Count data can be analyzed to identify new cell subtypes and to detect highly variable or differentially expressed (DE) genes between cell subpopulations.
This type of single-cell resolution is not possible with bulk RNA sequencing of cellular populations.
However, the downside is that the counts often contain high levels of technical noise with many ``drop-outs'', i.e., zero or near-zero values.
This is due to presence of low amounts of RNA per cell, which decreases the efficiency of capturing transcripts into libraries for sequencing.
Moreover, the capture efficiency often varies from cell to cell, such that counts cannot be directly compared between cells.

Normalization of the scRNA-seq counts is a critical step that corrects for differences in capture efficiency between cells.
Two broad classes of normalization approaches are available -- those using spike-in RNA sets, and those using the counts for cellular RNA.
In the former, the same quantity of spike-in RNA is added to each cell prior to library preparation \cite{stegle2015computational}.
Any difference in the coverage of the spike-in transcripts must be caused by differences in capture efficiency between cells.
Normalization is then performed by scaling the counts to equalize spike-in coverage between cells.
For the methods using the cellular counts, the assumption is that most genes are not DE across the sampled cells.
Counts are scaled so that there is, on average, no fold-difference in expression between cells for the majority of genes.
This is the underlying concept of commonly used methods such as DESeq \cite{anders2010differential} and trimmed mean of M-values (TMM) normalization \cite{robinson2010scaling}.
An even simpler approach involves scaling the counts to remove differences in library sizes between cells.

The type of normalization that can be used depends on the characteristics of the data set.
In some data sets, spike-in counts may not be present which obviously precludes their use in normalization.
For example, droplet-based protocols \cite{klein2015droplet,macosko2015highly} do not allow spike-ins to be easily incorporated.
Spike-in normalization also depends on several assumptions \cite{robinson2010scaling,marinov2014singlecell,grun2015design}, 
    the violations of which may compromise performance \cite{risso2014normalization}.
Methods based on cellular counts can be applied more generally but have their own deficiencies.
Normalization by library size is insufficient when DE genes are present, as composition biases can introduce spurious differences between cells \cite{robinson2010scaling}.
DESeq or TMM normalization are more robust to DE but rely on the calculation of ratios of counts between cells.
This is not straightforward in scRNA-seq data, where the high frequency of drop-out events interferes with stable normalization.
A large number of zeroes will result in nonsensical size factors from DESeq or undefined M-values from TMM.
One could proceed by removing the offending genes during normalization for each cell, but this may introduce biases if the number of zeroes varies across cells.

Correct normalization of scRNA-seq data is essential as it determines the validity of all downstream quantitative analyses.
In this article, we describe a deconvolution approach that improves the accuracy of non-DE normalization without using spike-ins.
Briefly, normalization is performed on pooled counts for multiple cells, where the incidence of problematic zeroes is reduced by summing across cells.
The pooled size factors are then deconvolved back to those for the individual cells.
Using a variety of simple simulations, we demonstrate that our approach outperforms the direct application of existing normalization methods for count data with many zeroes.
We also show a similar difference in behaviour on several real data sets, where the use of different normalization methods affects the final biological conclusions.
These results suggest that our approach is a viable alternative to existing methods for general normalization of scRNA-seq data.

\section{Existing normalization methods fail with zero counts}

\subsection{The origin of zero counts in scRNA-seq data}
The high frequency of zeroes in scRNA-seq data is driven by both biological and technical aspects.
Gene expression is highly variable across cells due to cell-to-cell heterogeneity and phenomena like transcriptional bursting \cite{marinov2014singlecell}.
Such variability is likely to result in zero counts for the lowly expressed genes.
It is also technically difficult to process low quantities of input RNA into sequenceable libraries.
This results in high dropout rates whereby low-abundance transcripts are not captured during library preparation \cite{brennecke2013accounting}.

At this point, it is important to distinguish between stochastic and systematic zeroes.
Systematic zeroes refer to genes that are constitutively silent in a cell (sub)population, such that the count must be zero for all cells in that population.
These are generally not problematic as they contain no information and can be removed prior to normalization.
Stochastic zeroes are found in genes that are actively expressed but obtain counts of zero in some cells due to sampling stochasticity.
These genes may contain information about the relative differences between cells, so removing them prior to normalization may introduce biases.

\subsection{A brief description of existing non-spike-in methods}
Here, we only consider those normalization methods not based on spike-in data.
This is motivated by the desire to obtain a general method that can be applied in all data sets.
In particular, we will review three approaches that are commonly used for RNA-seq data -- DESeq, TMM and library size normalization.

DESeq normalization was originally introduced as part of the DESeq package for detecting DE genes \cite{anders2010differential}.
It first constructs an ``average'' reference library, in which the ``count'' for each gene is defined as the geometric mean of the counts for that gene across all real libraries.
Each real library is then normalized against this average.
Specifically, for each gene, we compute the ratio of the count in each library to that in the average library.
The size factor for each library is defined as the median of this ratio across all genes.
This value represents the extent to which the counts in that library should be downscaled, 
    in order to eliminate any systematic differences in expression between libraries for the majority of (assumed) non-DE genes.

TMM normalization was introduced as part of the edgeR package for DE testing \cite{robinson2010edgeR}.
This method selects one library to be a reference, and normalizes each other library against the reference.
Specifically, for each library, M-values (i.e., library size-adjusted log$_2$-ratios in expression) are computed against the reference for all genes.
The genes with the most extreme M-values are trimmed away.
High- or low-abundance genes are similarly removed.
The weighted mean of the remaining M-values is computed, using precision weights that are calculated based on the asymptotic variance of each M-value.
This is used to define the normalization factor for each library as $2^x$, where $x$ is the weighted mean for that library.
The normalization factor represents the downscaling required to eliminate systematic differences between libraries, additional to that required to equalize the library sizes.
Taking the product of the normalization factor and library size for each library (i.e., the effective library size) yields a value that is functionally equivalent to the size factor.

Both DESeq and TMM normalization assume that most genes are not DE between libraries.
Any systematic difference in expression across the majority of genes is treated as bias.
This is duly incorporated into the size/normalization factors and removed upon scaling.
If the non-DE assumption does not hold, the computed factors will not be accurate.
In addition, both methods perform poorly in the presence of a large number of zeroes.
For DESeq normalization, the geometric mean will be equal to zero for genes with a zero count in any library, such that the ratios for that gene become undefined.
Conversely, a library with zero counts for a majority of genes will have a size factor of zero, which precludes any sensible scaling.
For TMM normalization, M-values are undefined when the count in either library is zero.
In such conditions, both methods typically require \textit{ad hoc} workarounds such as the removal of zero counts within each library.

Finally, library size normalization is another commonly used approach for normalizing RNA-seq data.
This involves scaling the counts such that the library size is the same across libraries,
    and is the basis for measures of normalized expression like counts- or transcripts-per-million.
While simple, this approach is not robust in the presence of DE genes \cite{robinson2010scaling}.
One can imagine a scenario where a single gene is strongly upregulated in one library compared to another.
This increases the size of the first library as more reads are generated from the upregulated gene.
Normalization by library size will compensate by scaling up the counts for the second library.
However, this will introduce spurious differences between libraries for all of the non-DE genes.
The likely presence of DE in real data means that library size normalization is often inappropriate.

\subsection{Performance of existing methods on simulated data with zeroes}
To test the performance of existing methods, simulated scRNA-seq data was generated with DE genes and a large number of stochastic zeros.
Consider a cell $j$ in a subpopulation $s$.
This subpopulation may represent cell type or some other biological condition, e.g., drug treatment. 
For each gene $i$ in this cell, the count $y_{ij}$ was sampled from a negative binomial (NB) distribution with mean $\theta_{j}\lambda_{is}$.
The $\theta_{j}$ term represents cell-specific biases (e.g., in capture efficiency) that must be normalized out, 
    and is sampled for each cell such that 
\[
\log_2(\theta_j) \sim \mathcal{N}(0, 0.25) \;.
\]
The $\lambda_{is}$ term denotes the expected number of transcripts of gene $i$ for cells in subpopulation $s$.
It is defined as $\lambda_{is}=\phi_{is}\lambda_{i0}$ where $\phi_{is}$ represents the DE for this gene in this subpopulation, and 
\[
\log_2(\lambda_{i0}) \sim \mbox{Uniform}(3, 6) \;.
\]
This recapitulates the spread of abundances in real data.
The NB dispersion is defined for each gene as 
\[
    \varphi_i = 2 + \frac{100}{\lambda_{i0}}
\]
to represent a decreasing mean-dispersion trend.
Large values for $\varphi_i$ are consistent with the high levels of technical noise and biological heterogeneity in scRNA-seq data,
    and ensure that a large number of stochastic zeroes are generated.
Approximately 30-50\% of all counts are sampled as zero in each simulated library.

The simulation design involved 10000 genes for three subpopulations of 250 cells each.
For each subpopulation, a unique set of $G$ genes was randomly chosen.
DE was introduced by setting $\phi_{is}$ to some constant $\phi_s$ for $p_s$ of the $G$ genes (i.e., upregulated) and to $0$ for the rest (downregulated).
The value of $\phi_s$ was set to 5 for all $s$, while $p_s$ was set to 20, 50 and 80\% for the first, second and third subpopulations, respectively.
This provides each subpopulation with a unique expression signature containing different numbers of DE genes in each direction.
Simulations were performed for $G = 0$ (no DE), 1000 (moderate DE) and 3000 (strong DE).
The simulation was also repeated with $G$ set to 3000, $p_s$ set to 50\% for all $s$ and $\phi_{s}$ set to 2, 5 and 10 for the first, second and third subpopulations, respectively.
This represents an alternative scenario where the number of DE genes in either direction is the same but the magnitude of DE is different between subpopulations.
In each simulation scenario, the count for each gene in each cell was sampled from the above NB distribution,
    the mean of which was modified according to the parameter settings of that scenario.

DESeq, TMM and library size normalization were applied to this data.
For DESeq normalization, the geometric mean was computed by replacing all zero counts with unity.
This is necessary to avoid a situation where a majority of genes have geometric means of zero, such that the majority of ratios to the geometric mean would be undefined.
Zero counts were also removed within each library prior to calculation of the median ratio, to avoid obtaining size factors equal to zero.
For TMM normalization, the calcNormFactors function in the edgeR package (v3.12.0) was used with default settings.
All undefined M-values were automatically removed prior to trimming and calculation of the normalization factor for each library.
The corresponding size factor was defined as the effective library size, i.e., the product of the library size and the normalization factor for each library.
For library size normalization, the total library size was used directly as the size factor for each cell.
The true size factor for each cell is $\theta_j$, as it represents the extent of scaling required to remove the cell-specific bias of $j$.
Estimated size factors were then compared to the true values for all cells.

All methods yield size factors that systematically deviate from the true values (Figure~\ref{fig:existing_sim}).
For DESeq and TMM normalization, large size factors are consistently underestimated while small size factors are overestimated.
This is a consequence of removing stochastic zeroes prior to normalization.
Cells with low $\theta_j$ are likely to contain more stochastic zeroes, as the mean of the sampling distribution for the counts is lower.
If these zeroes are removed prior to DESeq normalization, the median ratio will be computed from the remaining non-zero counts.
This shifts the median upwards and results in overestimation.
Similarly, the distribution of M-values will be shifted towards positive values upon removal of zeroes.
This is because stochastic zeroes represent sampled values below some non-zero mean, and would generally correspond to negative M-values.
Their removal increases the (trimmed) mean of M-values and biases the estimate of the TMM normalization factor.
The converse applies to cells with large $\theta_j$.
Recall that size factors have a relative interpretation across cells, so overestimation of small $\theta_j$ will lead to a concomitant underestimation for large $\theta_j$.

\begin{figure}[tbp]
\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm  5mm,clip]{results/size_1.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/size_2.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/size_3.pdf}
\includegraphics[width=\textwidth,trim=0mm  5mm 2mm 20mm,clip]{results/size_4.pdf}
\subcaption{}\label{subfig:size_sim}
\end{minipage}
\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm  5mm,clip]{results/TMM_1.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/TMM_2.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/TMM_3.pdf}
\includegraphics[width=\textwidth,trim=0mm  5mm 2mm 20mm,clip]{results/TMM_4.pdf}
\subcaption{}\label{subfig:tmm_sim}
\end{minipage}
\begin{minipage}{0.33\textwidth}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm  5mm,clip]{results/lib_1.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/lib_2.pdf}
\includegraphics[width=\textwidth,trim=0mm 14mm 2mm 20mm,clip]{results/lib_3.pdf}
\includegraphics[width=\textwidth,trim=0mm  5mm 2mm 20mm,clip]{results/lib_4.pdf}
\subcaption{}\label{subfig:lib_sim}
\end{minipage}
\caption{
    Performance of existing normalization methods on the simulated data with DE genes and stochastic zeroes.
    The size factor estimates for all cells are plotted against the true values for (\subref{subfig:size_sim}) DESeq,
        (\subref{subfig:tmm_sim}) TMM and (\subref{subfig:lib_sim}) library size normalization.
    Simulations were performed with no DE (first row), moderate DE (second row), strong DE (third row) and varying magnitudes of DE (fourth row).
    Axes are shown on a log-scale.
    For comparison, each set of size factors was scaled such that the same grand mean across cells was the same as that for the true values.
    The red line represents equality between the rescaled estimates and true factors.
    Cells in the first, second and third subpopulations are shown in black, blue and orange, respectively.
}
\label{fig:existing_sim}
\end{figure}

The presence of DE genes results in a further deterioration in performance of all methods (Figure~\ref{fig:existing_sim}).
The divergence between the true and estimated size factors increases as the number of DE genes increases,
    consistent with a decrease in the validity of the non-DE assumption required by all methods.
To illustrate, consider the strong DE simulation.
Across the three subpopulations, there are 9000 genes involved in subpopulation-specific signatures, i.e., 90\% of all genes exhibit DE within this data set.
An assumption of a non-DE majority of genes is clearly invalid in this scenario.
Any systematic differences in expression between cells of different subpopulations will now include genuine DE in addition to cell-specific biases.
Subsequently, the magnitude of DE will be incorporated in the size factors computed from each method, such that they are no longer accurate estimates of $\theta_j$.
The estimates also split into three distinct groups in Figure~\ref{fig:existing_sim}, each corresponding to a different subpopulation.
Size factors tend to be overestimated for subpopulations with more DE, as the increased expression of upregulated genes drags up the median ratio/trimmed mean.

It is worth noting that library size normalization is not affected by stochastic zeroes.
This is because the library size is stably computed by summing across all genes in each cell.
This results in improved performance in the simple simulation with no DE (Figure~\ref{subfig:lib_sim}).
However, as previously discussed, the use of the library size as a size factor assumes that all genes are non-DE in each cell.
Violations of this assumption lead to substantial estimation errors, which can be seen in the results for the simulations involving any DE.

\section{Improving normalization accuracy with deconvolution}

\subsection{Overview of the deconvolution strategy}
The aim of the deconvolution strategy is to normalize on summed expression values from pools of cells.
Summation across cells results in fewer zeroes, which means that the ensuing normalization is less susceptible to the errors observed for existing methods.
While normalization accuracy is improved, the estimated size factors are only relevant to the pools of cells.
This is not particularly interesting for downstream analyses focusing on single cells.
In order to be useful, the size factor for each pool is deconvolved back into the size factors for its constituent cells.
This ensures that cell-specific biases can be properly normalized.

The deconvolution method is centred around the fact that the size factor for the pool of cells can be expressed as a sum of the size factors for the constituent cells.
This allows a linear system to be constructed and solved, in order to obtain estimates for the cell-specific size factors.
Further implementation details for this method can be found in Section~\suppdeconv{} of the Supplementary Materials.
This includes an outline of the mathematical framework; 
    the use of linear inverse models to protect against negative estimates;
    the use of clustering to weaken the assumption of a non-DE majority of genes;
    and a description of the strategy used to choose the cells in each pool for construction of the linear system (see also Supplementary Figure~\suppring{}).

\subsection{Performance of the deconvolution approach on simulated data}
The deconvolution method provides accurate estimates of the size factor estimates in most simulation scenarios (Figure~\ref{fig:sim_cluster_DE}).
This is consistent with the reduced number of zero counts in the summed counts for each set of cells.
The median ratio for each set is more accurately computed, which improves the accuracy of the size factor estimates for the individual cells upon deconvolution.
Systematic under- or overestimation of the size factors for cells with large or small $\theta_j$ is avoided.
Some inaccuracy is observed in Figure~\ref{subfig:sumclust_3}, where the non-DE assumption is partially violated by large numbers of DE genes between subpopulations.
However, the deconvolution method is still more accurate than the existing methods in the third row of Figure~\ref{fig:existing_sim}
    -- unlike DESeq or TMM normalization, the estimates here are proportional to the true values within each subpopulation, 
       and the deviation from the diagonal is smaller than that for library size normalization.

\begin{figure}[btp]
\begin{center}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_1.pdf}
    \subcaption{}\label{subfig:sumclust_1}
\end{minipage}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_2.pdf}
    \subcaption{}\label{subfig:sumclust_2}
\end{minipage}  \\ 
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_3.pdf}
    \subcaption{}\label{subfig:sumclust_3}
\end{minipage}
\begin{minipage}{0.33\textwidth}
    \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{results/sumClust_4.pdf}
    \subcaption{}\label{subfig:sumclust_4}
\end{minipage}
\end{center}
    \caption{
        Size factor estimates from the deconvolution method in the simulation with DE genes and stochastic zeroes,
        shown against the true values for scenarios with (\subref{subfig:sumclust_1}) no DE, (\subref{subfig:sumclust_2}) moderate DE, 
                (\subref{subfig:sumclust_3}) strong DE and (\subref{subfig:sumclust_4}) varying magnitude of DE.
        Cells in the first, second and third subpopulations are shown in black, blue and orange, respectively.
        Axes are shown on a log-scale, and the red line represents equality with the true factors.
    }
    \label{fig:sim_cluster_DE}
\end{figure}

\section{Differences in normalization are recapitulated in real data}

\subsection{Overview of the data sets}
We also examined the behaviour of the deconvolution method in real scRNA-seq data.
The first data set was taken from a study of the somatosensory cortex and hippocampal region of the mouse brain \cite{zeisel2015brain}.
Libraries were prepared for over 3000 brain-derived single cells using the Fluidigm C1 system.
Gene expression was quantified for each cell by counting UMIs after sequencing.
Counts for all cells were obtained from \url{http://linnarssonlab.org/cortex}.
The second data set was generated using the inDrop protocol on mouse embryonic stem cells \cite{klein2015droplet}.
Libraries were prepared for over 10000 cells, and quantification was performed with UMIs.
Counts were obtained from the NCBI Gene Expression Omnibus with the accession GSE65525.

For both data sets, low-abundance genes were defined as those with an average count below 0.2 across all cells.
These were considered to be systematic zeroes (with some non-zero counts due to residual transcription, mapping errors, etc.) and removed prior to further analysis.
The set of External RNA Controls Consortium (ERCC) spike-ins in the brain data set was also removed.
This ensures that normalization is only performed using the counts for the cellular genes.
For the inDrop data set, counts were only used for cells before withdrawal of leukemia inhibitory factor (LIF), and those 7 days after withdrawal.
This resulted in a final set consisting of approximately 1700 cells.
Cells in the intervening time points were not considered as the library sizes were too small -- the average total count across all genes was around 3000 for each cell.

\subsection{Deconvolution yields a wider spread of size factors}
Each normalization method was applied on the counts for cells in both data sets.
In both cases, deconvolution yields a wider range of size factor estimates compared to DESeq and TMM normalization (Figure~\ref{fig:real_comp}).
This is consistent with the simulation results where the stochastic zeroes cause the estimates to be biased towards unity for these two existing methods.
Indeed, approximately 60\% of counts are equal to zero in each cell for both data sets, even after the removal of low-abundance genes.
This indicates that the presence of stochastic zeroes is an intrinsic property of high-throughput scRNA-seq data that cannot simply be ignored.
Recall that the deconvolution method reduces the number of zero counts by summing across cells.
This avoids bias towards unity in the simulations and increases the range of the estimates in the brain and inDrop data.

%\begin{figure}[btp]
%    \begin{minipage}{0.48\textwidth}
%        \includegraphics[width=\textwidth,trim=0mm 0mm 0mm 5mm,clip]{realdata/Zeisel_NormFactors.pdf}
%        \subcaption{}\label{subfig:spread_brain}
%    \end{minipage}
%    \begin{minipage}{0.48\textwidth}
%        \includegraphics[width=\textwidth,trim=0mm 0mm 0mm 5mm,clip]{realdata/Klein_NormFactors.pdf}
%        \subcaption{}\label{subfig:spread_indrop}
%    \end{minipage}
%    \caption{
%        Boxplots of the estimated size factors across all cells in the (\subref{subfig:spread_brain}) brain or (\subref{subfig:spread_indrop}) inDrop data,
%            for all normalization methods.
%        Axes are on a log-scale, and dots represent outlier cells with extreme size factors.
%        To simplify comparisons between methods, each set of size factors was scaled to have a median of unity.
%    }
%    \label{fig:real_spread}   
%\end{figure}

\begin{figure}[btp]
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_SFvDeconv.pdf}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Klein_SFvDeconv.pdf}
        \subcaption{}\label{subfig:comp_sf}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_TMMvDeconv.pdf}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Klein_TMMvDeconv.pdf}
    \subcaption{}\label{subfig:comp_tmm}
    \end{minipage}
    \begin{minipage}{0.33\textwidth}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Zeisel_LibvDeconv.pdf}
        \includegraphics[width=\textwidth,trim=0mm 5mm 0mm 15mm,clip]{realdata/Klein_LibvDeconv.pdf}
        \subcaption{}\label{subfig:comp_lib}
    \end{minipage}
    \caption{
        Comparisons between the estimated size factors from the deconvolution method to those from (\subref{subfig:comp_sf}) DESeq,  (\subref{subfig:comp_tmm}) TMM
            or (\subref{subfig:comp_lib}) library size normalization.
        This is shown for the brain (top) and inDrop data sets (bottom).
        Axes are on a log-scale, and the blue line represents equality between the two sets of factors.
        All sets of factors were centered to a median of unity.
        For the comparison between library size and deconvolution in the brain data, all cells except for oligodendrocytes (red) were used to compute the median for centering.
    }
    \label{fig:real_comp}  
\end{figure}

Deconvolution also yields different size factors from library size normalization in the brain data (Figure~\ref{subfig:comp_lib}).
Specifically, a substantial number of cells have size factor estimates from library size normalization that are larger than those from deconvolution.
This is attributable to the likely presence of DE genes between different cell types -- specifically, between oligodendrocytes and the other neuronal cells.
Any upregulation will increase the size factors from library size normalization for affected cells by (obviously) increasing the library sizes.
In contrast, deconvolution uses a median-based approach that is robust to extreme ratios caused by DE.
The two methods are more similar for the inDrop data where less DE is expected within the same cell type.

The brain data set also contains counts for ERCC spike-in genes.
In such cases, an alternative normalization strategy can be performed to equalize spike-in coverage across cells, 
    under the assumption that an equal amount of spike-in is added to each cell \cite{stegle2015computational}.
However, the size factors from spike-in normalization and those from deconvolution and the existing methods show no similarity (Supplementary Figure~\suppspikefig{}).
This is largely attributable to the differences in the assumptions between spike-in normalization and that of deconvolution and the other existing methods,
    as well as the insensitivity of the latter methods to differences in cell size and total RNA content.
A more detailed explanation is provided in Section~\suppspikesec{} of the Supplementary Materials.

\subsection{Different normalization schemes change the DE profile}
To gauge the impact of the differences in the normalization methods, a DE analysis was performed with edgeR (v3.12.0) on both data sets with the different size factor estimates.
For the brain data set, the count data was subsetted to contain only cells classified as microglia or oligodendrocytes \cite{zeisel2015brain}.
Size factors from each method were used as the effective library sizes.
The estimateDisp function was used to estimate a gene-specific NB dispersion for each gene \cite{chen2014differential} without any empirical Bayes shrinkage.
A generalized linear model was fitted for each gene using a one-way layout with the two cell types \cite{mccarthy2012differential}.
The glmTreat function was used to detect genes with a DE log-fold change significantly greater than 1 between the two cell types \cite{mccarthy2009testing}.
The Benjamini-Hochberg correction was applied to the $p$-values to control the false dicovery rate (FDR) at a threshold of 5\%.
This process was repeated for the inDrop data to test for DE after LIF withdrawal.

% With 1000 samples, you get so much evidence for DE that you call things with really small log-FCs. TREAT protects against this.

The greatest difference between methods is observed in the number of DE genes changing in each direction in the brain data set (Table~\ref{tab:real_de}).
Deconvolution yields a more balanced set of DE genes than DESeq or TMM normalization, with smaller differences between the numbers of up- and downregulated genes.
The set of DE genes detected in either direction also tends to be a superset or subset of that detected by the existing methods.
This is consistent with a difference in overall scaling of the average expression values between different biological conditions, 
    such that the DE log-fold changes from deconvolution are shifted in one direction relative to those from the existing methods.
The numbers for deconvolution are more similar to those for library size normalization, consistent with the relative similarity in Figure~\ref{fig:real_comp}.
Nonetheless, there are still 358 out of 1319 DE genes that are unique to deconvolution, 
    indicating that the systematic differences in the size factors for the oligodendrocytes are not negligible.
Smaller differences are observed for the inDrop data set where fewer DE genes are present.
Here, the similar performances of deconvolution and library size normalization are attributable to their mutual robustness to stochastic zeroes and, for the latter, a relative lack of DE.
This suggests that library size normalization may be satisfactory for homogeneous data sets.

\begin{table}[bt]
\caption{
    Number of DE genes detected by edgeR at a FDR of 5\% in each data set, using the size factor estimates from different methods for normalization.
    This is also separated into the number of up- and downregulated genes for each data set.
    Upregulation refers to increased expression in oligodendrocytes over microglia in the brain data, and to increased expression after LIF withdrawal in the inDrop data.
    The number of DE genes shared between analyses using deconvolution and each other method is also shown.
}
\begin{center}
\begin{tabular}{l r r r c r r r}
\hline
\multirow{2}{*}{\textit{Method}} & \multicolumn{3}{c}{\textit{Brain}} && \multicolumn{3}{c}{\textit{inDrop}}  \\
\cline{2-4}
\cline{6-8}
& Total & Down & Up && Total & Down & Up \\
\hline
DEseq                               & 1843 & 257 & 1586 && 497 & 298 & 199 \\
TMM                                 & 1757 & 273 & 1484 && 462 & 239 & 223 \\
Library size                        & 1237 & 641 & 596  && 492 & 199 & 293 \\
Deconvolution                       & 1319 & 503 & 816  && 489 & 212 & 277 \\
$\quad$ shared with DESeq           & 1073 & 257 & 816  && 411 & 212 & 199 \\
$\quad$ shared with TMM             & 1089 & 273 & 816  && 435 & 212 & 223 \\
$\quad$ shared with library size    & 1099 & 503 & 596  && 475 & 198 & 277 \\
\hline                                                   
\end{tabular}
\end{center}
\label{tab:real_de}
\end{table}

These results indicate that different normalization schemes will affect downstream steps like DE analyses.
For example, incorrect normalization may preserve (or even introduce) spurious differences in expression for non-DE genes by failing to properly correct for cell-specific biases.
Conversely, the log-fold changes for genuine DE genes may be distorted such that detection power is decreased.
In the data sets tested here, 20-30\% of DE genes are found or lost when the deconvolution method is used instead of the existing methods.
This is likely to have some impact on the biological conclusions that can be taken from the DE analysis.

\section{Conclusions}
Here, we have presented a normalization strategy for scRNA-seq data based on summation of expression values and deconvolution of pooled size factors.
This approach provides improved performance for size factor estimation compared to existing methods on simulated data.
In particular, it avoids estimation inaccuracy in the presence of stochastic zeroes and is robust to DE in the data set.
Similar differences in the size factors across methods were also observed in analyses of real data,
    where the use of different size factor sets resulted in changes to the number and identity of detected DE genes.
This indicates that the choice of normalization method has a substantive impact on the results of downstream analyses.
Any increase in accuracy from our deconvolution approach is likely to have some beneficial effect on the validity of the biological conclusions.
We have implemented our deconvolution approach as a R function, with C++ extensions for fast construction of the linear system.
It is available as part of the edgeR package in the Bioconductor project \cite{huber2015orchestrating}.

\bibliographystyle{unsrt}
\bibliography{references}

\end{document}

